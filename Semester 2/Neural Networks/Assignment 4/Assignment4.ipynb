{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8edfeb29",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 85\u001b[0m\n\u001b[1;32m     83\u001b[0m hidden_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m  \u001b[38;5;66;03m# Number of hidden units\u001b[39;00m\n\u001b[1;32m     84\u001b[0m model \u001b[38;5;241m=\u001b[39m CharLSTM(vocab_size, hidden_dim)  \u001b[38;5;66;03m# Initialize the model\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Step 6: Text generation function with handling missing characters\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_text\u001b[39m(model, start_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m, length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m):\n",
      "Cell \u001b[0;32mIn[1], line 74\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, encoded_text, epochs, batch_size, seq_length, lr)\u001b[0m\n\u001b[1;32m     71\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, y_batch\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m     72\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 74\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     75\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Clear the hidden state to prevent reusing the graph\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/IIT Patna/Letures/Semester 2/CS 582 Neural Network and NLP Lab/Assignment 2/nlpass2/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/IIT Patna/Letures/Semester 2/CS 582 Neural Network and NLP Lab/Assignment 2/nlpass2/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/IIT Patna/Letures/Semester 2/CS 582 Neural Network and NLP Lab/Assignment 2/nlpass2/lib/python3.11/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "path=\"Shakespeare_data.csv\"\n",
    "\n",
    "\n",
    "# Assuming text is loaded from the dataset\n",
    "# For the purpose of this example, let's simulate the text.\n",
    "text = open(path, 'r').read()  # Load the actual Shakespeare text here.\n",
    "\n",
    "# Step 1: Create character-to-index and index-to-character mappings\n",
    "chars = sorted(list(set(text)))  # Unique characters\n",
    "vocab_size = len(chars)  # Vocabulary size\n",
    "\n",
    "# Mapping characters to indices and vice versa\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Encode the text into integer sequence\n",
    "encoded_text = np.array([char_to_idx[ch] for ch in text])\n",
    "\n",
    "# Step 2: Define a function to create batches for training\n",
    "def get_batches(encoded_text, batch_size, seq_length):\n",
    "    total_batch_size = batch_size * seq_length\n",
    "    num_batches = len(encoded_text) // total_batch_size\n",
    "\n",
    "    encoded_text = encoded_text[:num_batches * total_batch_size]\n",
    "    x = encoded_text.reshape((batch_size, -1))  # Create a matrix of shape (batch_size, total_sequence_length)\n",
    "\n",
    "    for n in range(0, x.shape[1], seq_length):\n",
    "        x_batch = x[:, n:n+seq_length]\n",
    "        y_batch = np.roll(x_batch, -1, axis=1)  # Shift by one for the target\n",
    "        yield torch.tensor(x_batch, dtype=torch.long), torch.tensor(y_batch, dtype=torch.long)\n",
    "\n",
    "# Step 3: Define the character-level LSTM model\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(vocab_size, hidden_dim, batch_first=True)  # LSTM layer\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)  # Output layer to predict the next character\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out.reshape(-1, self.hidden_dim))  # Reshape to match output dimensions\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize hidden and cell states\n",
    "        return (torch.zeros(1, batch_size, self.hidden_dim),\n",
    "                torch.zeros(1, batch_size, self.hidden_dim))\n",
    "\n",
    "# Step 4: Define the training function\n",
    "def train(model, encoded_text, epochs=50, batch_size=1, seq_length=10, lr=0.001):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        hidden = model.init_hidden(batch_size)  # Initialize hidden state\n",
    "        total_loss = 0\n",
    "\n",
    "        # Iterate through batches\n",
    "        for x_batch, y_batch in get_batches(encoded_text, batch_size, seq_length):\n",
    "            x_onehot = nn.functional.one_hot(x_batch, num_classes=vocab_size).float()  # One-hot encoding\n",
    "\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            output, hidden = model(x_onehot, hidden)  # Forward pass\n",
    "\n",
    "            loss = criterion(output, y_batch.view(-1))  # Calculate loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            # Clear the hidden state to prevent reusing the graph\n",
    "            hidden = tuple([h.detach() for h in hidden])\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(encoded_text)}\")\n",
    "\n",
    "# Step 5: Train the model\n",
    "hidden_dim = 128  # Number of hidden units\n",
    "model = CharLSTM(vocab_size, hidden_dim)  # Initialize the model\n",
    "train(model, encoded_text, epochs=50, batch_size=1, seq_length=10, lr=0.001)  # Train the model\n",
    "\n",
    "# Step 6: Text generation function with handling missing characters\n",
    "def generate_text(model, start_str=\"hello\", length=100, temperature=0.7):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    chars = list(start_str)\n",
    "    hidden = model.init_hidden(1)  # Initialize hidden state\n",
    "\n",
    "    for ch in start_str:\n",
    "        if ch not in char_to_idx:\n",
    "            print(f\"Character '{ch}' not found in vocabulary. Skipping.\")\n",
    "            continue\n",
    "        x = torch.tensor([[char_to_idx[ch]]]).long()  # Convert character to index\n",
    "        x_onehot = nn.functional.one_hot(x, num_classes=vocab_size).float()  # One-hot encoding\n",
    "        output, hidden = model(x_onehot, hidden)  # Forward pass\n",
    "\n",
    "    # Generate characters one by one\n",
    "    for _ in range(length):\n",
    "        output_dist = nn.functional.softmax(output / temperature, dim=1).data  # Apply softmax and temperature\n",
    "        top_char = torch.multinomial(output_dist, 1)[0]  # Sample character from the distribution\n",
    "        chars.append(idx_to_char[top_char.item()])  # Append predicted character to the string\n",
    "\n",
    "        x_onehot = nn.functional.one_hot(top_char, num_classes=vocab_size).float().unsqueeze(0)  # One-hot encoding\n",
    "        output, hidden = model(x_onehot, hidden)  # Forward pass\n",
    "\n",
    "    return ''.join(chars)  # Return generated text\n",
    "\n",
    "# Generate some text\n",
    "generated_text = generate_text(model, start_str=\"hello\", length=200, temperature=0.7)\n",
    "print(\"Generated Text:\\n\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8969c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
